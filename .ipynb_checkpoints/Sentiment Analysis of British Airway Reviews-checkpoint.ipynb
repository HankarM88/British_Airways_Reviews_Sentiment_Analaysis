{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import re \n",
    "import string\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not Verified | I was meant to fly in January t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>✅ Trip Verified |  We have flown repeatedly wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>✅ Trip Verified |  I was horrified by the extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>✅ Trip Verified |  \\r\\nThe worst cabin experie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>✅ Trip Verified | First time flying with Briti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review\n",
       "0  Not Verified | I was meant to fly in January t...\n",
       "1  ✅ Trip Verified |  We have flown repeatedly wi...\n",
       "2  ✅ Trip Verified |  I was horrified by the extr...\n",
       "3  ✅ Trip Verified |  \\r\\nThe worst cabin experie...\n",
       "4  ✅ Trip Verified | First time flying with Briti..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data \n",
    "reviews = pd.read_csv(\"data/BA_Reviews.csv\")\n",
    "# show a sample of reviews \n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3542 entries, 0 to 3541\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  3542 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 27.8+ KB\n"
     ]
    }
   ],
   "source": [
    "reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaninf and Preprocessing\n",
    "\n",
    "The reviews texts at hand are very messy and before we progress with any analysis, we have to clean the texts from punctuations, digits, tags, http link, special characters, emojis, etc. After that we need to perform some prprocessing tasks such as tokenization, stopwords removal, stemming, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to clean the text \n",
    "def clean(text):\n",
    "    if '|' in text:\n",
    "        text = text.split('|')[1]\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('[^\\w\\s]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "def preprocess(text): \n",
    "    #get english stops words \n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #tokenize text \n",
    "    word_tokens = word_tokenize(text) \n",
    "    #remove stop words \n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
    "    #stemming the text \n",
    "    stemmed_text=[lemmatizer.lemmatize(word) for word in filtered_text]\n",
    "    # join text again\n",
    "    final_text = \" \".join(stemmed_text).strip()\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Preprocessed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not Verified | I was meant to fly in January t...</td>\n",
       "      <td>meant fly january algeria paid ticket day mean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>✅ Trip Verified |  We have flown repeatedly wi...</td>\n",
       "      <td>flown repeatedly british airway one world alli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>✅ Trip Verified |  I was horrified by the extr...</td>\n",
       "      <td>horrified extremely small seat poor training c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>✅ Trip Verified |  \\r\\nThe worst cabin experie...</td>\n",
       "      <td>worst cabin experience ever cramped seat low c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>✅ Trip Verified | First time flying with Briti...</td>\n",
       "      <td>first time flying british airway first time fl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  \\\n",
       "0  Not Verified | I was meant to fly in January t...   \n",
       "1  ✅ Trip Verified |  We have flown repeatedly wi...   \n",
       "2  ✅ Trip Verified |  I was horrified by the extr...   \n",
       "3  ✅ Trip Verified |  \\r\\nThe worst cabin experie...   \n",
       "4  ✅ Trip Verified | First time flying with Briti...   \n",
       "\n",
       "                                 Preprocessed_review  \n",
       "0  meant fly january algeria paid ticket day mean...  \n",
       "1  flown repeatedly british airway one world alli...  \n",
       "2  horrified extremely small seat poor training c...  \n",
       "3  worst cabin experience ever cramped seat low c...  \n",
       "4  first time flying british airway first time fl...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy the dataframe and launch cleaning and preprocessing tasks \n",
    "df = reviews.copy()\n",
    "df['Preprocessed_review'] = df['Review'].apply(clean).apply(preprocess)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Review:\n",
      " ✅ Trip Verified |  Prague to Denver via London. I almost missed my flight because at check-in I had to complete some really important papers that took 40 minutes -  later, no one asked me for those papers. Flight to Heathrow was average, what you expect from a 2 hour flight. At Heathrow the security lasted me  30 minutes. When I boarded the next the plane a lack of leg room, it was less than the flight before.  The flight then was pretty much okay, the food was delicious, had pasta with tomato sauce and several times drinks, the flight staff was kind and helpful. But the plane had no paid Wi-Fi, no outlets or USB ports, the screens were low quality.\n",
      "\n",
      "\n",
      " Preprocessed Review:\n",
      " prague denver via london almost missed flight checkin complete really important paper took minute later one asked paper flight heathrow average expect hour flight heathrow security lasted minute boarded next plane lack leg room le flight flight pretty much okay food delicious pasta tomato sauce several time drink flight staff kind helpful plane paid wifi outlet usb port screen low quality\n"
     ]
    }
   ],
   "source": [
    "# compare text and cleaned text \n",
    "from random import randrange\n",
    "n = randrange(len(df))\n",
    "print(\" Review:\\n\", df.Review[n])\n",
    "print('\\n')\n",
    "print(\" Preprocessed Review:\\n\", df.Preprocessed_review[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm having a wonderful time\n",
      "{'neg': 0.0, 'neu': 0.448, 'pos': 0.552, 'compound': 0.5719}\n"
     ]
    }
   ],
   "source": [
    "# define a function \n",
    "def analyzer(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    if sia.polarity_scores(text).compound>0:\n",
    "        return 'positive'\n",
    "    if sia.polarity_scores(text).compound<0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'netural'\n",
    "    \n",
    "\n",
    "\n",
    "# Text analysis example\n",
    "example = 'I\\'m having a wonderful time'\n",
    "print(example)\n",
    "print(sia.polarity_scores(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
